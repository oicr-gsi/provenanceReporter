# -*- coding: utf-8 -*-
"""
Created on Fri Jun 10 14:30:23 2022

@author: rjovelin
"""


import sqlite3
import json
import requests
import gzip
import argparse
import time
import traceback
import os
import subprocess
from utilities import connect_to_db
from whole_genome import get_workflow_limskeys, find_WGS_blocks


def extract_project_info(pinery):
    '''
    (str) -> dict
    
    Returns a dictionary with project information pulled down from Pinary API

    Parameters
    ----------
    - pinery (str): Pinery API, http://pinery.gsi.oicr.on.ca
    '''
    
    project_provenance = pinery + '/sample/projects'
    
    headers = {'accept': 'application/json',}
    response = requests.get(project_provenance, headers=headers)
    
    if response.ok:
        L = response.json()
    else:
        L = []
    
    D = {}
    
    if L:
        for i in L:
            name = i['name']
            assert name not in D
            D[name] = {'project_id': name}
            for j in ['pipeline', 'description', 'active', 'contact_name', 'contact_email']:
                if j in i:
                    D[name][j] = i[j] 
                else:
                    D[name][j] = ''
                if j == 'active' and j in i:
                    if i[j]:
                        D[name][j] = 'Active'
                    else:
                        D[name][j] = 'Completed'
    return D                            


def collect_info(data, names, keys):
    '''
    (dict, list, list) -> dict

    Returns a dictionary with keys by extracting specific information using names from data
    Note, the value of any name in data may be a string or single-element-list

    Parameters
    ----------
    - data (dict): Dictionary with data to be extracted
    - names (list): List of keys of interest in data
    - keys (list): List of renamed keys in output dictionary
    '''
    
    d = {}
    for j in range(len(names)):
        if names[j] in data:
            if type(data[names[j]]) == list:
                d[keys[j]] = data[names[j]][0]         
            else:
                d[keys[j]] = data[names[j]]
        else:
            d[keys[j]] = ''

    return d




def get_QC_status_from_nabu(project, workflow, api):
    '''
    (str, str, str) -> dict
    
    Returns a dictionary with qc information extracted from nabu for files generated by workflow in project
        
    Parameters
    ----------
    - project (str): Project of interest
    - workflow (str): Workflow of interest
    - api (str): URL of the Nabu api
    '''

    # get end-point
    api += 'get-fileqcs' if api[-1] == '/' else '/get-fileqcs'
    
    D = {project: {}}
    
    # check each fastq-generating workflow
    headers = {'accept': 'application/json','Content-Type': 'application/json'}
    json_data = {"project": "{0}".format(project), "workflow": workflow}
    response = requests.post(api, headers=headers, json=json_data)
    # check response code
    if response.status_code == 200:
        L = response.json()['fileqcs']
        if L:
            for i in L:
                qc = collect_info(i, ['skip', 'username', 'date', 'qcstatus', 'ref', 'stalestatus', 'comment'], ['skip', 'user', 'date', 'status', 'reference', 'fresh', 'ticket']) 
                file_swid = i['fileid']    
                D[project][file_swid] = qc
    return D    
    


def get_project_workflows(project, database, workflow_table = 'Workflows'):
    '''
    (str, str, str) -> list
    
    Returns a list of all workflows for a given project

    Parameters
    ----------
    - project (str): Project of interest
    - database (str): Path to the sqlite database
    - workflow_table (str): Name of table with workflow information. Default: Workflows
    '''

    # make a list of all workflows for a given project
    conn = sqlite3.connect(database)
    conn.row_factory = sqlite3.Row
    data = conn.execute('SELECT * FROM {0} WHERE project_id="{1}"'.format(workflow_table, project)).fetchall()
    conn.close()
    workflows = set()
    for i in data:
        i = dict(i)
        workflows.add(i['wf'])
    workflows = list(workflows)
    
    return workflows


def add_missing_QC_status(D, project, database, table = 'Files'):
    '''
    (dict, str, str, str) -> dict
    
    - D (dict): Dictionary with QC information for project files
    - project (str): Name of project of interest
    - database (str): Path to the database file
    - table (str): Table in database storing file information. Default is Files
    - nabu_api (str): URL of the nabu API
    '''
    
    # QC may not be available for all files
    # retrieve file swids from database instead of parsing again fpr
    # add empty values to qc fields if qc not available
    conn = sqlite3.connect(database)
    cur = conn.cursor()
    cur.execute('SELECT {0}.file_swid FROM {0} WHERE {0}.project_id = \"{1}\"'.format(table, project))
    records = cur.fetchall()
    records = [i[0] for i in records]
    conn.close()
    
    for file_swid in records:
        if file_swid not in D[project]:
            D[project][file_swid] = {'skip': '', 'user': '', 'date': '', 'status': '', 'reference': '', 'fresh': '', 'ticket': ''}
    
    return D
    

def collect_qc_info(project, database, nabu_api):
    '''
    (str, str, str, str) -> dict

    Returns a dictionary with QC status of all files in project    
        
    Parameters
    ----------
    - project (str): Name of project of interest
    - database (str): Path to the database file
    - nabu_api (str): URL of the nabu API
    '''

    # track qc info for all files in project
    D = {project: {}}

    # make a list of workflows for project
    workflows = get_project_workflows(project, database)
    for workflow in workflows:
        qc = get_QC_status_from_nabu(project, workflow, nabu_api)     
        # update dict
        D[project].update(qc[project])
    # add project files that may be missing QC info in Nabu
    D = add_missing_QC_status(D, project, database)

    return D


def collect_file_info_from_fpr(fpr, project_name):
    '''
    (str, str) -> dict

    Returns a dictionary with file information for project_name extracted from File Provenance Report
        
    Parameters
    ----------
    - fpr (str): Path to File Provenance Report file
    - project_name (str): Name of project of interest 
    '''

    infile = open_fpr(fpr)
    # skip header
    infile.readline()
        
    D = {}
    
    for line in infile:
        line = line.rstrip()
        if line:
            line = line.split('\t')
            # get project and initiate dict
            project = line[1]
            # keep only file info for project_name
            if project != project_name:
                continue             
            if project not in D:
                D[project] = {}
            
            # check if file is skipped
            # if line[51].lower() == 'true':
            #     continue
                   
            # get creation date
            creation_date = line[0]
            # convert creation date into epoch time
            # remove milliseconds
            creation_date = creation_date.split('.')[0]
            pattern = '%Y-%m-%d %H:%M:%S'
            creation_date = int(time.mktime(time.strptime(creation_date, pattern)))
            # get file path
            file = line[46]
            # get md5sums
            md5sum = line[47]
            # get file_swid
            file_swid = line[44]
            
            # keep only unique string. file swid doesn't match workflow inout file swids
            #file_swid = os.path.basename(file_swid)
            
            # get the library type
            library_type = line[17]
            if library_type:
                d = collect_info({k.split('=')[0]:k.split('=')[1] for k in line[17].split(';')},
                         ['geo_library_source_template_type'], ['library_type'])
                library_type = d['library_type']
            else:
                library_type = ''
                        
            # get file attributes
            attributes = line[45]
            if attributes:
                attributes = attributes.split(';')
                attributes = json.dumps({k.split('=')[0]: k.split('=')[1] for k in attributes})
            else:
                attributes = ''
            
            # get workflow
            workflow = line[30]
            # get workflow version
            version = line[31]        
            # get workflow run accession
            workflow_run = line[36]
            # get limskey
            limskey = line[56]
            
            # get skip and stale status
            skip = line[51]
            if skip.lower() == 'true':
                skip = 1
            else:
                skip = 0
            stale = line[52]
            
            # collect file info
            if file_swid not in D[project]:
                D[project][file_swid] = {'creation_date': creation_date, 'md5sum': md5sum,
                                         'workflow': workflow, 'version': version,
                                         'wfrun_id': workflow_run, 'file': file,
                                         'library_type': library_type, 'attributes': attributes,
                                         'limskey': [limskey], 'skip': skip, 'stale': stale}
            else:
                D[project][file_swid]['limskey'].append(limskey)
    infile.close()
    return D


def extract_projects_from_fpr(fpr):
    '''
    (str) -> list

    Returns a list of projects with data in FPR
        
    Parameters
    ----------
    - fpr (str): Path to File Provenance Report file
    '''

    infile = open_fpr(fpr)
    # skip header
    infile.readline()
        
    projects = set()
    
    for line in infile:
        line = line.rstrip()
        if line:
            line = line.split('\t')
            projects.add(line[1])
    infile.close()
    projects = list(projects)
    
    return projects



def get_valid_projects(projects, fpr):
    '''
    (dict, str) -> list
    
    Returns a list of projects defined in Pinery with data available in FPR
    
    Parameters
    ----------
    - projects (dict): Dictionary with project information extracted from project provenance
    - fpr (str): Path to the File Provenance Report
    '''
    
    # make a list of projects available in Pinery
    P = list(projects.keys())
    # list all projects with data in FPR
    L = extract_projects_from_fpr(fpr)
    # keep only projects listed in Pinery with data in FPR
    projects = list(set(P).intersection(set(L)))
   
    return projects


def filter_completed_projects(projects):
    '''
    (dict) -> dict
   
    Returns a dictionary in which only Active projects are kept (ie, Completed projects are removed)
   
    Parameters
    ----------
    projects (dict): Dictionary with project information extracted from project provenance
    '''

    to_remove = [i for i in projects if projects[i]['active'].lower() != 'active'] 
    for i in to_remove:
        del projects[i]
    return projects


def is_gzipped(file):
    '''
    (str) -> bool

    Return True if file is gzipped

    Parameters
    ----------
    - file (str): Path to file
    '''
    
    # open file in rb mode
    infile = open(file, 'rb')
    header = infile.readline()
    infile.close()
    if header.startswith(b'\x1f\x8b\x08'):
        return True
    else:
        return False


def open_fpr(fpr):
    '''
    (str) -> _io.TextIOWrapper
    
    Returns a file open for reading
    
    Parameters
    ----------
    - fpr (str): Path to File Provenance Report file
    '''

    # open provenance for reading. allow gzipped file or not
    if is_gzipped(fpr):
        infile = gzip.open(fpr, 'rt', errors='ignore')
    else:
        infile = open(fpr)
    return infile



def get_workflow_relationships(fpr, project_name):
    '''
    (str, str) -> (dict, dict, dict)

    Returns a tuple with dictionaries with worklow information, parent file ids for each workflow,
    and workflow id for each file from project of interest
    
    Parameters
    ----------
    - fpr (str): Path to the File Provenance Report
    - project_name (str): Name of project of interest
    '''

    F, P, W = {}, {}, {}

    # open fpr
    infile = open_fpr(fpr)
    # skip header
    infile.readline()
       
    for line in infile:
        line = line.rstrip()
        if line:
            line = line.split('\t')
            # get project name
            project = line[1]
            
            if project != project_name:
                continue
            
            # # skip any workflow/file that has a skipped file
            # if line[51].lower() == 'true':
            #     continue
                        
            # get workflow, workflow version and workflow run accession
            workflow, workflow_version, workflow_run = line[30], line[31], line[36]
            # get file swid
            file_swid = line[44]
            
            # get skip and stale status
            skip = line[51]
            if skip.lower() == 'true':
                skip = 1
            else:
                skip = 0
            stale = line[52]
            
            # keep only unique string. file swid doesn't match file swid of worklow input files
            #file_swid = os.path.basename(file_swid)
            
            input_files = line[38]
            if input_files:
                input_files = sorted(input_files.split(','))
            
                #input_files = sorted(list(set((map(lambda x: os.path.basename(x), input_files.split(','))))))
            else:
                input_files = []
            
            # get workflow attributes
            attributes = line[37]
            if attributes:
               attributes = attributes.split(';')
               attributes = json.dumps({k.split('=')[0]: k.split('=')[1] for k in attributes if k.split('=')[0] not in ['cromwell-workflow-id', 'major_olive_version']})
            else:
               attributes = ''                
       
            if project not in P:
                P[project] = {}
      
            if workflow_run in P[project]:
                assert P[project][workflow_run] == input_files
            else:
                P[project][workflow_run] = input_files
                
            if project not in W:
                W[project] = {}
            if workflow_run in W[project]:
                assert W[project][workflow_run] == {'wfrun_id': workflow_run, 'wfv': workflow_version,
                                                    'wf': workflow, 'attributes': attributes,
                                                    'skip': skip, 'stale': stale}
            else:
                W[project][workflow_run] = {'wfrun_id': workflow_run, 'wfv': workflow_version,
                                            'wf': workflow, 'attributes': attributes,
                                            'skip': skip, 'stale': stale}
        
            if project not in F:
                F[project] = {}
            if file_swid in F[project]:
                assert F[project][file_swid] == workflow_run
            else:
                F[project][file_swid] = workflow_run
    
    infile.close()
    
    return W, P, F        



def identify_parent_children_workflows(P, F):
    '''
    (dict, dict, dict) -> dict     
    
    Returns a dictionary of children: parents workflows relationsips for a given project
        
    Parameters
    ----------
    - P (dict): Input file ids for each workflow run id
    - F (dict): Map of file id and workflow id
    '''
    
    # parents record child-parent workflow relationships
    parents = {}
    
    for project in P:
        for workflow in P[project]:
            if P[project][workflow]:
                parent_workflows = sorted(list(set([F[project][i] for i in P[project][workflow] if i in F[project]])))
            else:
                parent_workflows = ['NA']
            if project not in parents:
                parents[project] = {}
            if workflow not in parents[project]:
                parents[project][workflow] = parent_workflows
            else:
                assert parents[project][workflow] == parent_workflows
    
    return parents

    
def get_provenance_data(provenance):
    '''
    (str) -> list
    
    Returns a list of dictionary with lims information for each library
    
    Parameters
    ----------
    - provenance (str): URL of the pinery provenance API 
    '''
    
    response = requests.get(provenance)
    if response.ok:
        L = response.json()
    else:
        L = []
    
    return L



def get_sample_info(pinery, project):
    '''
    (str, str, str) -> dict
    
    Return a dictionary with sample information, including samples not sequenced
    for each project
    
    Parameters
    ----------
    - pinery (str): Pinery API
    - project (str): Name of project
    '''
    
    provenance = pinery + '/samples'
    
    headers = {'accept': 'application/json',}
    params = {'project': project,}
    response = requests.get(provenance, params=params, headers=headers)

    cases = []
    if response.ok:
        L = response.json()
        cases = list(set(map(lambda x: '_'.join(x.split('_')[:2]), [i['name'] for i in L])))     
                
    return cases   
    

def get_parent_sample_info(pinery, project, sample_info):
    '''
    (str, str, str) -> dict
    
    Return a dictionary with sample information, including samples not sequenced
    for each project
    
    Parameters
    ----------
    - pinery (str): Pinery API
    - project (str): Name of project
    - sample_info (str): Path to the json file with sample information
    '''
    
    cases = get_sample_info(pinery, project)
    
    infile = open(sample_info)
    samples = json.load(infile)
    infile.close()
    
    L = [samples[i] for i in cases if i in samples]
        
    return L   





    

def extract_workflow_info(fpr, project_name):
    '''
    (str, str) -> dict
    
    Returns a dictionary with library input information for all workflows for project_name
    
    Parameters
    ----------
    - fpr (str): Path to the File Provenance Report
    - project_name (str): Name of project of interest
    '''

    # create a dict to store workflow input libraries
    D = {}

    infile = open_fpr(fpr)
    # skip header
    infile.readline()
    
    for line in infile:
        line = line.rstrip()
        if line:
            line = line.split('\t')
            # get project name
            project = line[1]
            # keep only info for project name
            if project != project_name:
                continue             
    
            # if line[51].lower() == 'true':
            #     continue
    
    
            # get sample name
            sample = line[7]
            # get workflow name and workflow run accession
            workflow, workflow_run = line[30], line[36]
                                  
            # get lane and run
            run, lane = line[18], line[24]            
            # get library and limskey
            library, limskey  = line[13], line[56]
            
            # get barcode and platform
            barcode, platform = line[27], line[22]
            
            d = {'run': run, 'lane': lane, 'library': library, 'limskey': limskey, 'barcode': barcode, 'platform': platform}
            
            if project not in D:
                D[project] = {}
            if workflow_run not in D[project]:
                D[project][workflow_run] = {}
            if sample not in D[project][workflow_run]:
                D[project][workflow_run][sample] = {'sample': sample, 'workflow': workflow, 'libraries': [d]}
            else:
                assert sample == D[project][workflow_run][sample]['sample']
                assert workflow == D[project][workflow_run][sample]['workflow']
                if d not in D[project][workflow_run][sample]['libraries']:
                    D[project][workflow_run][sample]['libraries'].append(d)
    return D            


def define_column_names():
    '''
    (None) -> dict

    Returns a dictionary with column names for each table in database
    '''

    # create dict to store column names for each table {table: [column names]}
    column_names = {'Workflows': ['wfrun_id', 'wf', 'wfv', 'project_id', 'attributes', 'file_count', 'lane_count', 'skip', 'stale'],
                    'Parents': ['parents_id', 'children_id', 'project_id'],
                    'Projects': ['project_id', 'pipeline', 'description', 'active', 'contact_name', 'contact_email', 'last_updated', 'expected_samples', 'sequenced_samples', 'library_types'],
                    'Files': ['file_swid', 'project_id', 'md5sum', 'workflow', 'version', 'wfrun_id', 'file', 'library_type', 'attributes', 'creation_date', 'limskey', 'skip', 'stale'],
                    'FilesQC': ['file_swid', 'project_id', 'skip', 'user', 'date', 'status', 'reference', 'fresh', 'ticket'],
                    'Libraries': ['library', 'sample', 'tissue_type', 'ext_id', 'tissue_origin',
                                  'library_type', 'prep', 'tissue_prep', 'sample_received_date', 'group_id', 'group_id_description', 'project_id'],
                    'Workflow_Inputs': ['library', 'run', 'lane', 'wfrun_id', 'limskey', 'barcode', 'platform', 'project_id'],
                    'Samples': ['case_id', 'donor_id', 'species', 'sex', 'miso', 'created_date', 'modified_date', 'project_id', 'parent_project'],
                    'WGS_blocks': ['project_id', 'case_id', 'samples', 'bmpp_anchor', 'workflows', 'name', 'date', 'release_status', 'complete', 'network']}
        
    return column_names


def define_column_types():
    '''
    (None) -> dict

    Returns a dictionary with column types for each table in database
    '''
    
    # create dict to store column names for each table {table: [column names]}
    column_types = {'Workflows': ['VARCHAR(572)', 'VARCHAR(128)', 'VARCHAR(128)', 'VARCHAR(128)', 'TEXT', 'INT', 'INT', 'INT', 'VARCHAR(128)'],
                    'Parents': ['VARCHAR(572)', 'VARCHAR(572)', 'VARCHAR(128)'],
                    'Projects': ['VARCHAR(128) PRIMARY KEY NOT NULL UNIQUE', 'VARCHAR(128)',
                                  'TEXT', 'VARCHAR(128)', 'VARCHAR(256)', 'VARCHAR(256)', 'VARCHAR(256)', 'INT', 'INT', 'VARCHAR(256)'],
                    'Files': ['VARCHAR(572) PRIMARY KEY NOT NULL UNIQUE', 'VARCHAR(128)',
                              'VARCHAR(256)', 'VARCHAR(128)', 'VARCHAR(128)',
                              'VARCHAR(572)', 'TEXT', 'VARCHAR(128)', 'TEXT', 'INT', 'VARCHAR(256)', 'INT', 'VARCHAR(128)'],
                    'FilesQC': ['VARCHAR(572) PRIMARY KEY NOT NULL UNIQUE', 'VARCHAR(128)',
                                'VARCHAR(128)', 'VARCHAR(128)', 'VARCHAR(128)',
                                'VARCHAR(128)', 'VARCHAR(128)', 'VARCHAR(128)', 'VARCHAR(128)'],
                    'Libraries': ['VARCHAR(256) PRIMARY KEY NOT NULL', 'VARCHAR(128)',
                                  'VARCHAR(128)', 'VARCHAR(128)', 'VARCHAR(128)',
                                  'VARCHAR(128)', 'VARCHAR(128)', 'VARCHAR(128)', 
                                  'VARCHAR(256)', 'VARCHAR(128)', 'VARCHAR(256)', 'VARCHAR(128)'],
                    'Workflow_Inputs': ['VARCHAR(128)', 'VARCHAR(256)', 'INTEGER', 'VARCHAR(572)', 
                                        'VARCHAR(128)', 'VARCHAR(128)', 'VARCHAR(128)', 'VARCHAR(128)'],
                    'Samples': ['VARCHAR(128) PRIMARY KEY NOT NULL', 'VARCHAR(256)', 'VARCHAR(256)', 'VARCHAR(128)', 'VARCHAR(572)', 'VARCHAR(128)', 'VARCHAR(128)', 'VARCHAR(128)', 'VARCHAR(128)'],
                    'WGS_blocks': ['VARCHAR(128)', 'VARCHAR(128)', 'VARCHAR(572)', 'VARCHAR(572)', 'TEXT', 'VARCHAR(256)', 'VARCHAR(128)', 'INT', 'INT', 'TEXT']}
                    
    
    return column_types


def create_table(database, table):
    '''
    (str, str) -> None
    
    Creates a table in database
    
    Parameters
    ----------
    - database (str): Name of the database
    - table (str): Table name
    '''
    
    # get the column names
    column_names = define_column_names()[table]
    # get the column types
    column_types = define_column_types()[table]    
    
    # define table format including constraints    
    table_format = ', '.join(list(map(lambda x: ' '.join(x), list(zip(column_names, column_types)))))


    if table  in ['Workflows', 'Parents', 'Files', 'FilesQC', 'Libraries', 'Workflow_Inputs', 'Samples', 'WGS_blocks']:
        constraints = '''FOREIGN KEY (project_id)
            REFERENCES Projects (project_id)
            ON DELETE CASCADE ON UPDATE CASCADE'''
        table_format = table_format + ', ' + constraints 
    
    if table == 'Parents':
        constraints = '''FOREIGN KEY (parents_id)
          REFERENCES Workflows (wfrun_id)
          ON DELETE CASCADE ON UPDATE CASCADE,
          FOREIGN KEY (children_id)
              REFERENCES Workflows (wfrun_id)
              ON DELETE CASCADE ON UPDATE CASCADE''' 
        table_format = table_format + ', ' + constraints + ', PRIMARY KEY (parents_id, children_id, project_id)'
    
    if table == 'Worklows':
        table_format = table_format + ', PRIMARY KEY (wfrun_id, project_id)'
    
    if table == 'WGS_blocks':
        constraints = '''FOREIGN KEY (case_id)
          REFERENCES Samples (case_id)
          ON DELETE CASCADE ON UPDATE CASCADE'''
        table_format = table_format + ', PRIMARY KEY (samples, bmpp_anchor)'
      
    if table == 'Files':
        constraints = '''FOREIGN KEY (wfrun_id)
            REFERENCES Workflows (wfrun_id)
            ON DELETE CASCADE ON UPDATE CASCADE,
            FOREIGN KEY (file_swid)
               REFERENCES FilesQC (file_swid)
               ON DELETE CASCADE ON UPDATE CASCADE'''
        table_format = table_format + ', ' + constraints
    
    if table == 'Workflow_Inputs':
        constraints = '''FOREIGN KEY (wfrun_id)
            REFERENCES Workflows (wfrun_id)
            ON DELETE CASCADE ON UPDATE CASCADE,
            FOREIGN KEY (library)
              REFERENCES Libraries (library)
              ON DELETE CASCADE ON UPDATE CASCADE'''
        table_format = table_format + ', ' + constraints
    
    if table == 'Samples':
        constraints = '''FOREIGN KEY (donor_id)
            REFERENCES Libraries (ext_id)
            ON DELETE CASCADE ON UPDATE CASCADE'''
        table_format = table_format + ', ' + constraints

    if table == 'Libraries':
        constraints = '''FOREIGN KEY (sample)
            REFERENCES Samples (case_id)
            ON DELETE CASCADE ON UPDATE CASCADE'''
        table_format = table_format + ', ' + constraints

    # connect to database
    conn = sqlite3.connect(database)
    cur = conn.cursor()
    # create table
    cmd = 'CREATE TABLE {0} ({1})'.format(table, table_format)
    cur.execute(cmd)
    conn.commit()
    conn.close()



def initiate_db(database):
    '''
    (str) -> None
    
    Create tables in database
    
    Parameters
    ----------
    - database (str): Path to the database file
    '''
    
    # check if table exists
    conn = sqlite3.connect(database)
    cur = conn.cursor()
    cur.execute("SELECT name FROM sqlite_master WHERE type='table';")
    tables = cur.fetchall()
    tables = [i[0] for i in tables]    
    conn.close()
    for i in ['Projects', 'Workflows', 'Parents', 'Files', 'FilesQC', 'Libraries', 'Workflow_Inputs', 'Samples', 'WGS_blocks']:
        if i not in tables:
            create_table(database, i)


def remove_table(database, table):
    '''
    (str, str) -> None
    
    Drop table in database
    
    Parameters
    ----------
    - database (str): Path to the database file
    - table (str): Table of interest
    '''
    
    conn = sqlite3.connect(database)
    cur = conn.cursor()
    cur.execute("DROP TABLE {0}".format(table))
    conn.commit()
    conn.close()


def add_project_info_to_db(database, pinery, project, lims_info, table = 'Projects'):
    '''
    (str, str, str, str, str) -> None
    
    Add project information into Projects table of database
       
    Parameters
    ----------
    - database (str): Path to the database file
    - pinery (str): Pinery API, http://pinery.gsi.oicr.on.ca
    - project (str): Name of project of interest
    - lims_info (str): Path to the json file with lims information
    - table (str): Name of Table in database. Default is Projects
    '''
    
    # get project info
    projects = extract_project_info(pinery)
    projects = {project: projects[project]}
    
    # get column names
    column_names = define_column_names()[table]

    # add time stamp when project data was updated
    projects[project]['last_updated'] = time.strftime('%Y-%m-%d_%H:%M', time.localtime(time.time()))
    
    # add number of expected cases for project
    samples = get_sample_info(pinery, project)
    projects[project]['expected_samples'] = len(set(samples))
    
    # add number of sequenced cases for project
    infile = open(lims_info)
    lims = json.load(infile)
    infile.close()
    projects[project]['sequenced_samples'] = len(lims[project].keys())
    
    # add library types
    library_type = []
    for i in lims[project]:
        for j in lims[project][i]:
            library_type.append(lims[project][i][j]['library_type'])
    library_type = ','.join(sorted(list(set(library_type))))        
    projects[project]['library_types'] = library_type
        
    # connect to db
    conn = sqlite3.connect(database,  timeout=30)
    cur = conn.cursor()
    
    # order values according to column names
    vals = [projects[project][i] for i in column_names]
    # insert project info
    cur.execute('INSERT INTO {0} {1} VALUES {2}'.format(table, tuple(column_names), tuple(vals)))
    conn.commit()
    
    conn.close()




def add_workflows(workflows, database, project_name, table = 'Workflows'):
    '''
    (dict, str, str) -> None
    
    Inserts or updates workflow information to table Workflows in database
           
    Parameters
    ----------
    - workflows (dict): Dictionary with workflow information
    - database (str): Path to the database file
    - project_name (str): Name of project of interest
    - workflow_table (str): Name of the table storing workflow information. Default is Workflows
    '''
    
    # get column names
    column_names = define_column_names()[table]
    
    # connect to db
    conn = sqlite3.connect(database, timeout=30)
    cur = conn.cursor()
    
    for workflow_run in workflows[project_name]:
        # insert data into table
        values = [workflows[project_name][workflow_run][i] for i in column_names if i in workflows[project_name][workflow_run]]
        values.insert(3, project_name)
        
        cur.execute('INSERT INTO {0} {1} VALUES {2}'.format(table, tuple(column_names), tuple(values)))
        conn.commit()
        
    conn.close()


def add_workflow_relationships(parent_workflows, database, project, table = 'Parents'):    
    '''
    (dict, str, str, str) -> None
    
    Inserts or updates parent-children workflow relatiionships to table Parents in database
    
    Parameters
    ----------    
    - D (dict): Dictionary with children-parents workflow relationships 
    - database (str): Path to the database file
    - project (str): name of project of interest
    - table (str): Name of the table storing parents-children workflow relationships
    '''
    
    # get column names
    column_names = define_column_names()[table]
    
    # connect to db
    conn = sqlite3.connect(database, timeout=30)
    cur = conn.cursor()
    
    for workflow in parent_workflows[project]:
        for parent in parent_workflows[project][workflow]:
            # insert data into table
            cur.execute('INSERT INTO {0} {1} VALUES {2}'.format(table, tuple(column_names), (parent, workflow, project)))
            conn.commit()
                            
    conn.close()




def count_files(project_name, database, file_table = 'Files'):
    '''
    (str, str, str) -> dict
    
    Returns a dictionary with the number of files for each workflow in project
    
    Parameters
    ----------
    - project_name (str): Name of project of interest
    - database (str): Path to the sqlite database
    - file_table (str): Name of the table with File information
    '''
    
    conn = connect_to_db(database)
    data = conn.execute("SELECT DISTINCT {0}.file, {0}.wfrun_id FROM {0} WHERE {0}.project_id = '{1}'".format(file_table, project_name)).fetchall()
    conn.close()
    
    counts = {}
    for i in data:
        counts[i['wfrun_id']] = counts.get(i['wfrun_id'], 0) + 1
       
    return counts


def update_workflow_information(project, workflows, D, key):
    '''
    (str, dict, dict, str) -> None
    
    Update in place the dictionary workflows for a given project using key to store information contains in dictionary D
    
    Parameters
    ----------
    - project (str): Project of interest
    - workflows (dict): Dictionary containing workflow information for a given project
    - D (dictionary): Dictionary containing file count or amount of input sequencing lanes for each workflow of project
    - key (str): Key use to store the information from D in workflows.
                 Valid keys are file_count or lane_count
    '''
    
    for workflow_id in D:
        workflows[project][workflow_id][key] = D[workflow_id]
    

def add_workflows_info_to_db(fpr, database, project_name, workflow_table = 'Workflows', parent_table = 'Parents', children_table = 'Children', file_table = 'Files', workflow_input_table = 'Workflow_Inputs'):
    '''
    (str, str, str, str, str, str, str) -> None
    
    Inserts or updates workflow information and parent-children workflow relationships
 
    Parameters
    ----------    
    - fpr (str): Path to the File Provenance Report
    - database (str): Path to the database file
    - project_name (str): Name of project of interest
    - workflow_table (str): Name of the table storing workflow information. Default is Workflows
    - parent_table (str): Name of the table storing parents-children workflow relationships. Default is Parents
    - children_table (str): Name of the table storing children-parents workflow relationships. Default is Children
    - file_table (str): Name of the table storing file information. Default is Files
    - workflow_input_table (str): Name of the table storing the workflow input information. Default is Workflow_Inputs
    '''

    # get the workflow inputs and workflow info
    workflows, parents, files = get_workflow_relationships(fpr, project_name)
    
    # get the file count for each workflow
    counts = count_files(project_name, database, file_table)
    # update workflow information with file count
    update_workflow_information(project_name, workflows, counts, 'file_count')
    
    # get the amount of data for each workflow
    limskeys = get_workflow_limskeys(project_name, database, workflow_input_table)
    for i in limskeys:
        limskeys[i] = len(limskeys[i])
    # update workflow information with lane count
    update_workflow_information(project_name, workflows, limskeys, 'lane_count')
        
    # check that project is defined in FPR (ie, may be defined in Pinery but no data recorded in FPR)
    if workflows and parents and files:
        # create a dict {workflow: [parent workflows]}
        parent_workflows = identify_parent_children_workflows(parents, files)

        # add workflow info
        add_workflows(workflows, database, project_name, workflow_table)
        
        # add parents-children workflow relationships to Parents table
        add_workflow_relationships(parent_workflows, database, project_name, parent_table)    
       

    
def add_fileQC_info_to_db(database, project, fpr, nabu_api, table='FilesQC'):
    '''
    (str, str, str, str, str) -> None
    
    Inserts file QC information in database's FilesQC table
       
    Parameters
    ----------
    - database (str): Path to the database file
    - project (str): Name of project of interest
    - fpr (str): Path to the File Provenance Report
    - nabu_api (str): URL of the nabu API
    - table (str): Table in database storing the QC or file information. Default is FilesQC
    '''

    # collect QC info from nabu
    D = collect_qc_info(project, database, nabu_api)
    
    # check that data is recorded in nabu for project
    if D:
        # connect to db
        conn = sqlite3.connect(database)
        cur = conn.cursor()
        
        # get column names
        column_names = define_column_names()[table]

        # add data
        for file_swid in D[project]:
            L = [D[project][file_swid][i] for i in column_names if i in D[project][file_swid]]
            L.insert(0, project)
            L.insert(0, file_swid)
            cur.execute('INSERT INTO {0} {1} VALUES {2}'.format(table, tuple(column_names), tuple(L)))
            conn.commit()
    
        conn.close()



def add_file_info_to_db(database, project, fpr, nabu_api, table = 'Files'):
    '''
    (str, str, str, str, str, str, str) -> None
    
    Inserts or updates file QC information in database's FilesQC table
       
    Parameters
    ----------
    - database (str): Path to the database file
    - project (str): Name of project of interest
    - fpr (str): Path to the File Provenance Report
    - file_table (str): Table in database storing file information. Default is Files 
    - nabu_api (str): URL of the nabu API
    - table (str): Table in database storing file information. Default is Files
    '''

    # collect file info from FPR
    D = collect_file_info_from_fpr(fpr, project)
    
    # get column names
    column_names = define_column_names()[table]

    # check that data is recorded in FPR for project
    if D:
        # connect to db
        conn = sqlite3.connect(database)
        cur = conn.cursor()
        
        # add data
        for file_swid in D[project]:
            D[project][file_swid]['limskey'] = ';'.join(list(set(D[project][file_swid]['limskey'])))
            L = [D[project][file_swid][i] for i in column_names if i in D[project][file_swid]]
            L.insert(0, project)
            L.insert(0, file_swid)
            cur.execute('INSERT INTO {0} {1} VALUES {2}'.format(table, tuple(column_names), tuple(L)))
            conn.commit()
    
        conn.close()



def add_library_info_to_db(database, project, pinery, lims_info, table = 'Libraries'):
    '''
    (str, str, str, str) -> None
    
    Inserts or updates library information in Libraries table of database    
    
    Parameters
    ----------
    - database (str): Path to the databae file
    - project (str): Name of project of interest
    - pinery (str): URL of the sample provenance API: 
    - lims_info (str): Path to the json file with lims information
    - table (str): Table storing library in database. Default is Libraries
    '''
    
    # collect lims information
    infile = open(lims_info)
    lims = json.load(infile)
    infile.close()
        
    # check that data is recorded for that project
    if project in lims and lims[project]:
        lims = {project: lims[project]}
    
        # get column names
        column_names = define_column_names()[table]
        
        # connect to db
        conn = sqlite3.connect(database)
        cur = conn.cursor()
        # add data
        for sample in lims[project]:
            for library in lims[project][sample]:
                L = [lims[project][sample][library][i] for i in column_names if i in lims[project][sample][library]]
                L.insert(0, sample)
                L.insert(0, library)
                L.append(project)
                # insert project info
                cur.execute('INSERT INTO {0} {1} VALUES {2}'.format(table, tuple(column_names), tuple(L)))
                conn.commit()
        conn.close()



def add_workflow_inputs_to_db(database, fpr, project, table = 'Workflow_Inputs'):
    '''
    (str, str, str) -> None
    
    Inserts or updates workflow input library information in table Workflow_Inputs of database    
    
    Parameters
    ----------
    - database (str): Path to the databae file
    - fpr (str): Path to the File Provenance Report
    - project (str): Name of project of interest
    - table (str): Table storing library in database. Default is Libraries
    '''

    # collect information about library inputs
    libraries = extract_workflow_info(fpr, project)
    
    # check that data is recorded in FPR for project
    if libraries:
        # connect to db
        conn = sqlite3.connect(database)
        cur = conn.cursor()
       
        # get column names
        data = cur.execute("SELECT * FROM {0} WHERE project_id = '{1}';".format(table, project))
        column_names = [column[0] for column in data.description]
    
        # add or update data
        for workflow_run in libraries[project]:
            for sample in libraries[project][workflow_run]:
                for i in libraries[project][workflow_run][sample]['libraries']:
                    L = []
                    for j in column_names:
                        if j in i:
                            if j != 'lane':
                                L.append(i[j])
                            else:
                                L.append(int(i[j]))
                    L.append(project)
                    L.insert(3, workflow_run)
                    
                    # insert project info
                    cur.execute('INSERT INTO {0} {1} VALUES {2}'.format(table, tuple(column_names), tuple(L)))
                    conn.commit()
     
        conn.close()



def add_samples_info_to_db(database, project, pinery, table, sample_info):
    '''
    (str, str, str, str) -> None
    
    Inserts samples data into Samples table of database    
    
    Parameters
    ----------
    - database (str): Path to the databae file
    - project (str): Name of project of interest
    - pinery (str): Pinery API
    - table (str): Name of table in database
    - sample_info (str): Path to the json file with sample information
    '''
    
    # collect information about samples
    samples = get_parent_sample_info(pinery, project, sample_info)
    
    if samples:
        # connect to db
        conn = sqlite3.connect(database)
        cur = conn.cursor()
   
        # get column names
        data = cur.execute("SELECT * FROM {0} WHERE project_id = '{1}';".format(table, project))
        column_names = [column[0] for column in data.description]

        # add data into table
        for i in samples:
            
            L = [i['case'], i['donor_id'], i['species'], i['sex'], i['miso'],
                 i['created_date'], i['modified_date'], project, i['project']]          
            
            # insert project info
            cur.execute('INSERT INTO {0} {1} VALUES {2}'.format(table, tuple(column_names), tuple(L)))
            conn.commit()
 
        conn.close()


def add_WGS_blocks_to_db(database, project, table):
    '''
    (str, str, str, str) -> None
    
    Inserts WGS blocks data into WGS_blocks table of database    
    
    Parameters
    ----------
    - database (str): Path to the databae file
    - project (str): Name of project of interest
    - table (str): Name of table in database
    '''
    
    # get the WGS blocks for donors in project
    blocks = find_WGS_blocks(project, database)

    if blocks:
        # connect to db
        conn = sqlite3.connect(database)
        cur = conn.cursor()
   
        # get column names
        data = cur.execute("SELECT * FROM {0} WHERE project_id = '{1}';".format(table, project))
        column_names = [column[0] for column in data.description]

        # add data into table
        for d in blocks:
            # loop over samples and blocks
            for samples in d:
                for block in d[samples]:
                    L = [d[samples][block]['project_id'],
                         d[samples][block]['case_id'],
                         samples,
                         block,
                         ';'.join(d[samples][block]['workflows']),
                         d[samples][block]['name'],
                         d[samples][block]['date'],
                         d[samples][block]['release'],
                         d[samples][block]['complete'],
                         d[samples][block]['network']]
            
                    # insert project info
                    cur.execute('INSERT INTO {0} {1} VALUES {2}'.format(table, tuple(column_names), tuple(L)))
                    conn.commit()
 
        conn.close()



def collect_lims_info(args):
    '''
    (str, str) -> str
    
    Parse sample-provenance in Pinery and writes information as json
    
    Parameters
    ----------
    - pinery (str): Pinery API
    - lims_info (str): Path to the output json file with lims information
    ''' 

    sample_provenance = args.pinery + '/provenance/v9/sample-provenance'
    # get sample info from pinery
    L = get_provenance_data(sample_provenance)
    
    # store lims information for each library of each project
    # {project: {sample: {library_info}}}    
    D = {}

    for i in L:
        project = i['studyTitle']
        sample = i['rootSampleName']  
        if project not in D:
            D[project] = {}
        if sample not in D[project]:
            D[project][sample] = {}
    
        # collect sample information
        d = collect_info(i['sampleAttributes'], ['geo_tissue_type', 'geo_external_name', 'geo_tissue_origin',
                 'geo_library_source_template_type', 'geo_prep_kit',
                 'geo_tissue_preparation', 'geo_receive_date', 'geo_group_id', 'geo_group_id_description'], ['tissue_type', 'ext_id', 'tissue_origin', 'library_type', 
                         'prep', 'tissue_prep', 'sample_received_date', 'group_id', 'group_id_description'])
        # add library name
        library = i['sampleName']
                        
        # store sample information
        if library not in D[project][sample]:
            D[project][sample][library] = d
            
        #update sample information for each library if some fields are missing
        for k in d:
            if D[project][sample][library][k] == '':
                D[project][sample][library][k] = d[k]    
    
    newfile = open(args.lims_info, 'w')
    json.dump(D, newfile)
    newfile.close()    



def collect_parent_sample_info(args):
    '''
    (str, str) -> str
    
    Parse sample-provenance in Pinery and writes information as json
    
    Parameters
    ----------
    - pinery (str): Pinery API
    - samples_info (str): Path to the output json file with sample information
    ''' 

    provenance = args.pinery + '/samples'
    # get sample info from pinery
    L = get_provenance_data(provenance)
    
    D = {}

    for i in L:
        if i['name'].count('_') == 1:
            case = i['name']
            sex, species, donor_id = '', '', ''
            for j in i['attributes']:
                if j['name'] == 'Sex':
                    sex = j['value']
                if j['name'] == 'Organism':
                    species= j['value']
                if j['name'] == 'External Name':
                    donor_id = j['value']
            sample_id = i['id']
            miso = 'https://miso.oicr.on.ca/miso/sample/{0}'.format(sample_id.replace('SAM', ''))    
            project = i['project_name']
            modified_date = i['modified_date']
            created_date = i['created_date']
            D[case] = {'project': project, 'sex': sex, 'species': species,
                       'miso': miso, 'donor_id': donor_id, 'case': case, 'modified_date': modified_date,
                       'created_date': created_date}
        
    newfile = open(args.samples_info, 'w')
    json.dump(D, newfile)
    newfile.close()    
    
    
    
def add_info(args):
    '''
    (list) -> None
    
    Adds infornation for a given project to the provenance reporter database
    
    Parameters
    ----------
    - fpr (str): Path to Path to the File Provenance Report
    - nabu (str): URL of the Nabu API
    - pinery (str): Pinery api
    - database (str): Path to the database file
    - lims_info (str): Path to the json file with lims information
    - project (str): Name of the project
    '''
    
    # create database if file doesn't exist
    if os.path.isfile(args.database) == False:
        initiate_db(args.database)
    
    # add project information    
    add_project_info_to_db(args.database, args.pinery, args.project, args.lims_info, 'Projects')
    print('added projects')
    
    # add sample information
    add_samples_info_to_db(args.database, args.project, args.pinery, 'Samples', args.samples_info)
    print('added samples') 
    
    
    # add file info
    add_file_info_to_db(args.database, args.project, args.fpr, args.nabu, 'Files')
    print('added files')
    
    
    # add workflow input
    add_workflow_inputs_to_db(args.database, args.fpr, args.project, 'Workflow_Inputs')
    print('added workflow inputs')
    
    
    
    # add workflow information
    add_workflows_info_to_db(args.fpr, args.database, args.project, 'Workflows', 'Parents', 'Children', 'Files', 'Workflow_Inputs')
    print('added workflows')
    
    # add file QC info
    add_fileQC_info_to_db(args.database, args.project, args.fpr, args.nabu, 'FilesQC')
    print('added filesqc')
    
    # add library information
    add_library_info_to_db(args.database, args.project, args.pinery, args.lims_info, 'Libraries')
    print('added libraries')
    
    # add WGS blocks
    add_WGS_blocks_to_db(args.database, args.project, 'WGS_blocks')
    print('added wgs blocks')  
      


def launch_jobs(args):
    '''
    (list) -> None
    
    Launch qsub jobs to fill the provenance reporter database     
    
    Parameters
    ----------
    - fpr (str): Path to Path to the File Provenance Report
    - nabu (str): URL of the Nabu API
    - pinery (str): Pinery API
    - database (str): Path to the database file
    - workingdir (str): Name of the directory where qsubs scripts are written
    - mem (int): Memory allocated to jobs
    '''
    
    # populate database with project information
    # extract project information from project provenance
    projects = extract_project_info(args.pinery)
    # filter out completed projects
    projects = filter_completed_projects(projects)
    # make a list of projects with data in Prinery and FPR
    projects = get_valid_projects(projects, args.fpr)
    
    # make a directory to save the scripts
    qsubdir = os.path.join(args.workingdir, 'qsubs')
    os.makedirs(qsubdir, exist_ok=True)
    # create a log dir
    logdir = os.path.join(qsubdir, 'log')
    os.makedirs(logdir, exist_ok=True)
    # make a directory to store the project databases
    databasedir = os.path.join(args.workingdir, 'databases')
    os.makedirs(databasedir, exist_ok=True)
    
    
    dbfiller = os.path.join(args.workingdir, 'prov_reporter_db_filler.py')

    cmd1 = '/u/rjovelin/SOFT/anaconda3/bin/python3.6 {0} collect_lims -p {1} -l {2}'
    bashScript = os.path.join(qsubdir, 'collect_lims.sh')
    lims_info_file = os.path.join(args.workingdir, 'lims_info.json')
    with open(bashScript, 'w') as newfile:
        newfile.write(cmd1.format(dbfiller, args.pinery, lims_info_file))
    qsubCmd = "qsub -b y -P gsi -l h_vmem={0}g -N {1}  -e {2} -o {2} \"bash {3}\"".format(args.mem, 'provdb.lims', logdir, bashScript)
    subprocess.call(qsubCmd, shell=True)


    cmd2 = '/u/rjovelin/SOFT/anaconda3/bin/python3.6 {0} collect_samples -p {1} -s {2}'
    bashScript = os.path.join(qsubdir, 'collect_samples.sh')
    samples_info_file = os.path.join(args.workingdir, 'samples_info.json')
    with open(bashScript, 'w') as newfile:
        newfile.write(cmd2.format(dbfiller, args.pinery, samples_info_file))
    qsubCmd = "qsub -b y -P gsi -l h_vmem={0}g -N {1}  -e {2} -o {2} \"bash {3}\"".format(args.mem, 'provdb.samples', logdir, bashScript)
    subprocess.call(qsubCmd, shell=True)

    cmd3 = '/u/rjovelin/SOFT/anaconda3/bin/python3.6 {0} add_project -f {1} -n {2} -p {3} -d {4} -pr {5} -l {6} -s {7}'
    
    # record job names and job exit codes    
    job_names = []

    for project in projects:
        database = os.path.join(databasedir, '{0}.db'.format(project))
        # get name of output file
        bashScript = os.path.join(qsubdir, '{0}_add_project_info.sh'.format(project))
        with open(bashScript, 'w') as newfile:
            newfile.write(cmd3.format(dbfiller, args.fpr, args.nabu, args.pinery, database, project, lims_info_file, samples_info_file))
        # launch qsub directly, collect job names and exit codes
        jobName = '{0}.provdb'.format(project)
        qsubCmd = 'qsub -b y -P gsi -l h_vmem={0}g,h_rt={1}:0:0 -N {2} -hold_jid "{3}" -e {4} -o {4} "bash {5}"'.format(args.mem, args.run_time, jobName, 'provdb.lims,provdb.samples', logdir, bashScript)
        subprocess.call(qsubCmd, shell=True)
        # store job names
        job_names.append(jobName)
    
    # launch job to copy database to server
    cmd4 = '/u/rjovelin/SOFT/anaconda3/bin/python3.6 {0} migrate -md {1} -jn "{2}" -wd {3} -pf {4} -s {5}'
    bashScript = os.path.join(qsubdir, 'migrate_db.sh')
    with open(bashScript, 'w') as newfile:
        newfile.write(cmd4.format(dbfiller, args.merged_database, ','.join(job_names), args.workingdir, args.pemfile, args.server))
    qsubCmd = "qsub -b y -P gsi -l h_vmem={0}g,h_rt={1}:0:0 -N {2}  -hold_jid \"{3}\" -e {4} -o {4} \"bash {5}\"".format(args.mem, args.run_time, 'provdb.migration', ','.join(job_names), logdir, bashScript)
    subprocess.call(qsubCmd, shell=True)



def collect_project_info(database, table):
    '''
    (str, str) -> list
    
    Returns a list of sqlite3.Row extracted from table in database
        
    Parameters
    ----------
    - database (str): Path to sqlite database
    - table (str): Table of interest in database
    '''
    
    # collect information from project database for table
    conn = sqlite3.connect(database)
    conn.row_factory = sqlite3.Row
    data = conn.execute('SELECT * FROM {0}'.format(table)).fetchall()
    conn.close()
    
    return data       
    
    
def update_database(merged_database, table, data):
    '''
    (str, str, list) -> None
    
    Update table in merged_database with data for a given project
    
    Parameters
    ----------
    - merged_database (str): Path to the merged database
    - table (str): Table of interest in database
    - data (list): list of sqlite3.Row extracted from table in project database
    '''
    
    # open merged database, add project data into table
    conn = sqlite3.connect(merged_database,  timeout=30)
    cur = conn.cursor()
    # get the column names
    c = list(zip(*list(dict(data[0]).items())))[0]
    for i in data:
        # get values in order of column names    
        v = list(zip(*list(dict(i).items())))[1]
        cur.execute('INSERT INTO {0} {1} VALUES {2}'.format(table, c , v))
        conn.commit()            
    conn.close()



def merge_two_databases(db1, db2):
    '''
    (str, str) -> None

    Merge database db2 into database db1
    Precondition: the two dababases have the same schema
    
    Parameters
    ---------
    - db1 (str): Path to database 1
    - db2 (str): Path to database 2
    '''

    # connect to db1  
    conn = sqlite3.connect(db1, timeout=30)
    # attach database db2
    conn.execute("ATTACH '" + db2 +  "' as dba")
    conn.execute("BEGIN")
    # loop over rows in selected info from db2
    for row in conn.execute("SELECT * FROM dba.sqlite_master WHERE type='table'"):
        # combine data into db1
        combine = "INSERT OR IGNORE INTO "+ row[1] + " SELECT * FROM dba." + row[1]
        conn.execute(combine)
    conn.commit()
    # detach from db2
    conn.execute("detach database dba")
    conn.close()
       


def merge_databases(merged_database, databases):
    '''
    (str, list) -> None    
    
    Merge all the project databases located into merged_database
    
    Parematers
    ----------
    - merged_database (str): Path to the merged database
    - databases (list): List of paths to the project databases
    '''
    
    start = time.time()
    
    # initiate merged database if file doesn't exist
    if os.path.isfile(merged_database) == False:
        initiate_db(merged_database)
    
    # remove tables
    conn = sqlite3.connect(merged_database)
    cur = conn.cursor()
    cur.execute("SELECT name FROM sqlite_master WHERE type='table';")
    tables = cur.fetchall()
    tables = [i[0] for i in tables]
    for table in tables:
        # drop table and re-initiate table
        remove_table(merged_database, table)
        create_table(merged_database, table)
              
    for db in databases:
        print(os.path.basename(db))
        merge_two_databases(merged_database, db)
    
    end = time.time()
    print('merged databases', end - start)


def get_job_exit_status(job):
    '''
    (str) -> int
    
    Returns the most recent exit status of job

    Parameters
    ----------
    - job (str): Job name     
    '''    

    try:
        output = subprocess.check_output('qacct -j {0}'.format(job), shell=True).decode('utf-8').rstrip().split('\n')
    except:
        output = ''
               
    if output:
        # record all exit status, the same job may have run multiple times
        d = {}
        for i in output:
            if 'jobname' in i:
                jobname = i.split()[1]
                assert job == jobname
            elif 'end_time' in i:
                j = i.split()[1:]
                if len(j) != 0:
                    # convert to epoch time
                    if '.' in j[1]:
                        j[1] = j[1][:j[1].index('.')]
                    date = '.'.join([j[0].split('/')[1], j[0].split('/')[0], j[0].split('/')[-1]]) + ' ' + j[1]
                    p = '%d.%m.%Y %H:%M:%S'
                    date = int(time.mktime(time.strptime(date, p)))
                else:
                    date = 0
            elif 'exit_status' in i:
                d[date] = i.split()[1]
        
        # get the most recent exit status        
        end_jobs = list(d.keys())
        end_jobs.sort()
        exit_status = int(d[end_jobs[-1]])
    else:
        exit_status = 1
    
    return exit_status


    
def migrate(args):
    '''
    (list) -> None
    
    Launch job to copy the database to the server
    
    Parameters
    ----------
    - database (str): Path to the database file
    - mem (str): Memory allocated to jobs
    - server (str): Server running the application
    - job_names (str): Semi-colon separated list of job names 
    '''
    
    
    # check if jobs are still running
    jobs = list(map(lambda x: x.strip(), args.job_names.split(',')))
    
    # make a list of successfully updated project db
    updated = []
    databasedir = os.path.join(args.workingdir, 'databases')
    for job in jobs:
        # get exit status
        exit_status = get_job_exit_status(job)
        if exit_status == 0:
            db =  os.path.join(databasedir, job.split('.')[0] + '.db')
            updated.append(db)
            
    # merge all projects databases that were successfully updated
    merge_databases(args.merged_database, updated)

    # copy merged database to server
    subprocess.call('scp -i {0} {1} {2}:~/provenance-reporter/'.format(args.pemfile, args.merged_database, args.server), shell=True)

    # remove project databases
    project_databases = [os.path.join(databasedir, i) for i in os.listdir(databasedir) if '.db' in i]
    for i in project_databases:
        assert '/scratch2/groups/gsi/bis/rjovelin/provenance_reporter/databases' in i and '.db' in i
        os.remove(i)
        



if __name__ == '__main__':

    
    # create top-level parser
    parent_parser = argparse.ArgumentParser(prog = 'prov_reporter_db_filler.py', description='Script to add data to the Provenance Reporter database', add_help=False)
    parent_parser.add_argument('-f', '--fpr', dest='fpr', default = '/scratch2/groups/gsi/production/vidarr/vidarr_files_report_latest.tsv.gz', help='Path to the File Provenance Report. Default is /scratch2/groups/gsi/production/vidarr/vidarr_files_report_latest.tsv.gz')
    parent_parser.add_argument('-n', '--nabu', dest='nabu', default='https://nabu-prod.gsi.oicr.on.ca', help='URL of the Nabu API. Default is https://nabu-prod.gsi.oicr.on.ca')
    parent_parser.add_argument('-p', '--pinery', dest = 'pinery', default = 'http://pinery.gsi.oicr.on.ca', help = 'Pinery API. Default is http://pinery.gsi.oicr.on.ca')
        
    main_parser = argparse.ArgumentParser(prog = 'prov_reporter_db_filler.py', description = 'Add data to the Provenance Reporter database')
    subparsers = main_parser.add_subparsers(title='sub-commands', description='valid sub-commands', dest= 'subparser_name', help = 'sub-commands help')
    
    # add project info  
    project_parser = subparsers.add_parser('add_project', help="Add project information to the Provenance Reporter database", parents = [parent_parser])
    project_parser.add_argument('-pr', '--project', dest='project', help='Name of the project of interest', required = True)
    project_parser.add_argument('-d', '--database', dest='database', help='Path to the database file', required=True)
    project_parser.add_argument('-l', '--lims', dest='lims_info', help='Path to json file with lims info', required=True)
    project_parser.add_argument('-s', '--samples', dest='samples_info', help='Path to json file with samples info', required=True)
    project_parser.set_defaults(func=add_info)
 
    # launch jobs to fill db with all projects info
    fill_parser = subparsers.add_parser('fill_db', help="Run jobs to fill database with information about all active projects.", parents = [parent_parser])
    fill_parser.add_argument('-wd', '--workingdir', dest='workingdir', help='Name of the directory where qsubs scripts are written', required = True)
    fill_parser.add_argument('-m', '--memory', dest='mem', default=20, help='Memory allocated to jobs')
    fill_parser.add_argument('-md', '--merged_database', dest='merged_database', help='Path to the merged database', required = True)
    fill_parser.add_argument('-rt', '--run_time', dest='run_time', default=5, help='Run time in hours')
    fill_parser.add_argument('-pf', '--pem_file', dest='pemfile', default='~/.ssh/provenance_reporter.pem', help='Path to the pem file to access the server')
    fill_parser.add_argument('-s', '--server', dest='server', help='Provenance reporter server.', required=True)
    fill_parser.set_defaults(func=launch_jobs)


    # collect lims information from pinery
    fill_parser = subparsers.add_parser('collect_lims', help="Collect lims information from Pinery.", parents = [parent_parser])
    fill_parser.add_argument('-l', '--lims', dest='lims_info', help='Path to json file with lims info', required = True)
    fill_parser.set_defaults(func=collect_lims_info)

    # collect sample information from pinery
    fill_parser = subparsers.add_parser('collect_samples', help="Collect sample information from Pinery.", parents = [parent_parser])
    fill_parser.add_argument('-s', '--samples', dest='samples_info', help='Path to json file with sample info', required = True)
    fill_parser.set_defaults(func=collect_parent_sample_info)

    # launch jobs to fill db with all projects info
    migrate_parser = subparsers.add_parser('migrate', help="Run job to copy the database to the server", parents=[parent_parser])
    migrate_parser.add_argument('-m', '--memory', dest='mem', default=20, help='Memory allocated to jobs')
    migrate_parser.add_argument('-rt', '--run_time', dest='run_time', default=5, help='Run time in hours')
    migrate_parser.add_argument('-jn', '--job_names', dest='job_names', help='Names of the jobs launched to fill the database')
    migrate_parser.add_argument('-wd', '--workingdir', dest='workingdir', help='Name of the directory where qsubs scripts are written', required = True)
    migrate_parser.add_argument('-md', '--merged_database', dest='merged_database', help='Path to the merged database', required = True)
    migrate_parser.add_argument('-pf', '--pem_file', dest='pemfile', default='~/.ssh/provenance_reporter.pem', help='Path to the pem file to access the server')
    migrate_parser.add_argument('-s', '--server', dest='server', help='Provenance reporter server.', required=True)
    migrate_parser.set_defaults(func=migrate)
    
    # get arguments from the command line
    args = main_parser.parse_args()
 
    args.func(args)
    

